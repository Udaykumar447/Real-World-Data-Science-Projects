{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Home Kitchen Product Reviews Analysis\n",
    "\n",
    "\n",
    "Data Source: http://jmcauley.ucsd.edu/data/amazon/index_2014.html\n",
    "\n",
    "The Amazon Home Kitchen Product Reviews dataset consists of reviews of home and kitchen products from Amazon website.<br>\n",
    "\n",
    "Number of reviews: 551,682<br>\n",
    "Timespan: May 1996 - July 2014<br>\n",
    "Number of Attributes/Columns in data: 9\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. reviewerId - unqiue identifier of the reviewer\n",
    "2. asin - unique identifier for the product\n",
    "3. reviewerName\n",
    "4. Helpfulness numerator and Helpfulness denominator\n",
    "   HelpfulnessNumerator - number of users who found the review helpful\n",
    "   HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "5. reviewText - text of the review\n",
    "6. overall - the overall rating of the reviewer\n",
    "7. summary - brief summary of the review\n",
    "8. unixReviewTime - timestamp for the review\n",
    "9. Time - Date of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "* Determining the polarity of the review (whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2)) using the reviews given by the user.\n",
    "\n",
    "#### Ground truth \n",
    "* We will use Overall score to determine the ground truth of the review. If the score is 4 or 5 , we will consider that review as positive review. If the score is 1 or 2 , we will consider that review as negative review. we will ignore the reviews with the rating of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available is in .json file form in data source link and we converted that into .csv file using the code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Home_and_Kitchen_5.json.gz')\n",
    "\n",
    "df.to_csv('amazon_home_kitchen_product_data', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('amazon_home_kitchen_product_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin            reviewerName   helpful  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz    [0, 0]   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh    [0, 0]   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews  [26, 27]   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"  [14, 18]   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0   \n",
       "1  I bought this zoku quick pop for my daughterr ...      5.0   \n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "3  This book is a must have if you get a Zoku (wh...      5.0   \n",
       "4  This cookbook is great.  I have really enjoyed...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  \n",
       "0  10 19, 2013  \n",
       "1  06 18, 2014  \n",
       "2   05 5, 2013  \n",
       "3   08 4, 2011  \n",
       "4   06 7, 2014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displyaing the first few rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551682, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the data before filtering the score rating 3\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the data by removing the overall rating score - 3 \n",
    "filtered_data = data[data.overall!= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506623, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the data after filtering the score rating 3\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the overall rating column to positive and negative categories\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "actualScore = filtered_data['overall']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['overall'] = positiveNegative\n",
    "\n",
    "# changing the overall column name to score for better understanding\n",
    "filtered_data = filtered_data.rename(columns={\"overall\": \"score\", \"asin\":\"productId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>productId</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID   productId            reviewerName   helpful  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz    [0, 0]   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh    [0, 0]   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews  [26, 27]   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"  [14, 18]   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover    [0, 0]   \n",
       "\n",
       "                                          reviewText     score  \\\n",
       "0  My daughter wanted this book and the price on ...  positive   \n",
       "1  I bought this zoku quick pop for my daughterr ...  positive   \n",
       "2  There is no shortage of pop recipes available ...  positive   \n",
       "3  This book is a must have if you get a Zoku (wh...  positive   \n",
       "4  This cookbook is great.  I have really enjoyed...  positive   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  \n",
       "0  10 19, 2013  \n",
       "1  06 18, 2014  \n",
       "2   05 5, 2013  \n",
       "3   08 4, 2011  \n",
       "4   06 7, 2014  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displyaing the first few rows of filtered data\n",
    "filtered_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the helpful column to helpful numerator and helpful denominator\n",
    "helpfulness_num =[]\n",
    "helpfulness_denom =[]\n",
    "for i in filtered_data['helpful']:\n",
    "    m = i[1:-1]\n",
    "    k = m.split(',')\n",
    "    helpfulness_num.append(k[0]);\n",
    "    helpfulness_denom.append(k[1]);\n",
    "\n",
    "filtered_data['helpfulnessNumerator'] = helpfulness_num          # adding helpfulness numerator column\n",
    "filtered_data['helpfulnessDenominator'] = helpfulness_denom      # adding helpfulness denominator column\n",
    "\n",
    "filtered_data['helpfulnessNumerator'] = pd.to_numeric(filtered_data[\"helpfulnessNumerator\"])\n",
    "filtered_data['helpfulnessDenominator'] = pd.to_numeric(filtered_data[\"helpfulnessDenominator\"])\n",
    "\n",
    "del filtered_data['helpful']                                     # deleting helpful column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>productId</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpfulnessNumerator</th>\n",
       "      <th>helpfulnessDenominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID   productId            reviewerName  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover   \n",
       "\n",
       "                                          reviewText     score  \\\n",
       "0  My daughter wanted this book and the price on ...  positive   \n",
       "1  I bought this zoku quick pop for my daughterr ...  positive   \n",
       "2  There is no shortage of pop recipes available ...  positive   \n",
       "3  This book is a must have if you get a Zoku (wh...  positive   \n",
       "4  This cookbook is great.  I have really enjoyed...  positive   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  helpfulnessNumerator  helpfulnessDenominator  \n",
       "0  10 19, 2013                     0                       0  \n",
       "1  06 18, 2014                     0                       0  \n",
       "2   05 5, 2013                    26                      27  \n",
       "3   08 4, 2011                    14                      18  \n",
       "4   06 7, 2014                     0                       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displyaing the first few rows of filtered data\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check \n",
    "duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to remove the duplicate reviews text (if any) as it was necessary to remove duplicates in order to get unbiased results for the analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('productId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506623, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"reviewerID\",\"reviewerName\",\"unixReviewTime\",\"reviewText\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows does not change after removing duplicates which shows that there are no duplicate reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check \n",
    "if there are any data points where helpfulnessNumerator is higher than helpfulnessDenominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>productId</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpfulnessNumerator</th>\n",
       "      <th>helpfulnessDenominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [reviewerID, productId, reviewerName, reviewText, score, summary, unixReviewTime, reviewTime, helpfulnessNumerator, helpfulnessDenominator]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final.helpfulnessNumerator>final.helpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "There are no data points where helpfulnessNumerator is higher than helpfulnessDenominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    455204\n",
       "negative     51419\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the number of positive and negative reviews\n",
    "final['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID                object\n",
       "productId                 object\n",
       "reviewerName              object\n",
       "reviewText                object\n",
       "score                     object\n",
       "summary                   object\n",
       "unixReviewTime             int64\n",
       "reviewTime                object\n",
       "helpfulnessNumerator       int64\n",
       "helpfulnessDenominator     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Text Preprocessing: Stemming, stop-word removal and Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6581\n",
      "About the temperatures it can handle (which for some reason aren't on the product description; you'd think this would be a key bullet point):Max recommended temp: 392F.  400F seems to be the extreme max according to the packaging, but > 392F will deteriorate the life span faster, says the instructions.  I've seen it go as low as 39F for refridgerated water; it could probably handle lower.  There's a switch on the bottom to measure in Celcius.  392F may be low for some people, so be warned.  I wonder if other reviewers with problems have been pushing this limit?  If the probe is off by as much as 10 degrees as some claim, that might explain the short lifespans.I've liked this probe so far, although admittedly I haven't used it yet for its primary purpose (measuring internal meat temperatures).  Instead I've used it as a kitchen timer and oven thermometer to monitor the oven air temps when proofing dough in it (don't want to kill my yeast!) by lodging the probe in a rack.  I haven't compared it to another thermometer/probe, so I don't know how calibrated it is, but it seems to be reasonable for my purposes.  For those of you without metal oven doors like me, it rests nicely on the countertop.  I do find the wire a bit gangly.  Oh, and the alarm is loud (which can be good or bad).\n"
     ]
    }
   ],
   "source": [
    "# printing the text example to check for punctuation\n",
    "final['reviewText'] = final[\"reviewText\"].astype('str')\n",
    "import re\n",
    "i=0;\n",
    "for sent in final['reviewText'].values:\n",
    "    if (len(re.findall('.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\envs\\ten\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\envs\\ten\\lib\\site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\envs\\ten\\lib\\site-packages (from nltk) (4.48.2)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\envs\\ten\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dell\\anaconda3\\envs\\ten\\lib\\site-packages (from nltk) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['reviewText'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags using the cleanhtml function\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>productId</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>score</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpfulnessNumerator</th>\n",
       "      <th>helpfulnessDenominator</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daughter want book price amazon best alreadi t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bought zoku quick pop daughterr zoku quick mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>shortag pop recip avail free web purchas zoku ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>book must get zoku also high recommend larg va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cookbook great realli enjoy review recip sure ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID   productId            reviewerName  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover   \n",
       "\n",
       "                                          reviewText     score  \\\n",
       "0  My daughter wanted this book and the price on ...  positive   \n",
       "1  I bought this zoku quick pop for my daughterr ...  positive   \n",
       "2  There is no shortage of pop recipes available ...  positive   \n",
       "3  This book is a must have if you get a Zoku (wh...  positive   \n",
       "4  This cookbook is great.  I have really enjoyed...  positive   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  helpfulnessNumerator  helpfulnessDenominator  \\\n",
       "0  10 19, 2013                     0                       0   \n",
       "1  06 18, 2014                     0                       0   \n",
       "2   05 5, 2013                    26                      27   \n",
       "3   08 4, 2011                    14                      18   \n",
       "4   06 7, 2014                     0                       0   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  daughter want book price amazon best alreadi t...  \n",
       "1  bought zoku quick pop daughterr zoku quick mak...  \n",
       "2  shortag pop recip avail free web purchas zoku ...  \n",
       "3  book must get zoku also high recommend larg va...  \n",
       "4  cookbook great realli enjoy review recip sure ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the first few rows of the column after adding cleanedText column\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (506623, 109473)\n",
      "the number of unique words  109473\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "final_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Grams and n-Grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words :  [(b'use', 376632), (b'one', 240634), (b'like', 180411), (b'great', 167067), (b'make', 151994), (b'work', 148127), (b'get', 144973), (b'well', 142686), (b'time', 122941), (b'good', 122636), (b'easi', 121049), (b'would', 120577), (b'love', 119267), (b'clean', 114567), (b'look', 111218), (b'need', 94779), (b'realli', 93427), (b'nice', 92397), (b'littl', 90573), (b'set', 90205)]\n",
      "Most Common Negative Words :  [(b'use', 41371), (b'one', 33417), (b'get', 23222), (b'would', 22322), (b'like', 21090), (b'work', 20304), (b'time', 19202), (b'product', 15966), (b'make', 15301), (b'dont', 13662), (b'look', 13294), (b'even', 13170), (b'good', 12995), (b'tri', 12955), (b'coffe', 12286), (b'water', 12190), (b'well', 11831), (b'back', 11769), (b'review', 11240), (b'thing', 11187)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (506623, 4098533)\n",
      "the number of unique words including both unigrams and bigrams  4098533\n"
     ]
    }
   ],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "count_vect = CountVectorizer(ngram_range=(1,2) ) #in scikit-learn\n",
    "final_bigram_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "**************************************************************************\n",
      "the shape of out text TFIDF vectorizer  (506623, 4098533)\n",
      "**************************************************************************\n",
      "the number of unique words including both unigrams and bigrams  4098533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print('**************************************************************************')\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print('**************************************************************************')\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['alway findpiec', 'alway fine', 'alway finest', 'alway finger', 'alway fingertip', 'alway finicki', 'alway finish', 'alway fire', 'alway firm', 'alway first']\n"
     ]
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "print(\"some sample features(unique words in the corpus)\",features[100000:100010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zoku quick</td>\n",
       "      <td>0.478449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zoku</td>\n",
       "      <td>0.409519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pop daughterr</td>\n",
       "      <td>0.271762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daughterr</td>\n",
       "      <td>0.263565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daughterr zoku</td>\n",
       "      <td>0.263565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quick maker</td>\n",
       "      <td>0.257749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bought zoku</td>\n",
       "      <td>0.249552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quick pop</td>\n",
       "      <td>0.203686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>love fun</td>\n",
       "      <td>0.199884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>maker love</td>\n",
       "      <td>0.180019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fun make</td>\n",
       "      <td>0.173512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>quick</td>\n",
       "      <td>0.153587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>make ice</td>\n",
       "      <td>0.144049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ice cream</td>\n",
       "      <td>0.122350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cream</td>\n",
       "      <td>0.109916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.104427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pop</td>\n",
       "      <td>0.099035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ice</td>\n",
       "      <td>0.093253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>maker</td>\n",
       "      <td>0.091205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bought</td>\n",
       "      <td>0.059113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>love</td>\n",
       "      <td>0.052943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>make</td>\n",
       "      <td>0.050136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>zzzzzzzzzzzzzzzzz night</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>follow go</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>follow goal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature     tfidf\n",
       "0                zoku quick  0.478449\n",
       "1                      zoku  0.409519\n",
       "2             pop daughterr  0.271762\n",
       "3                 daughterr  0.263565\n",
       "4            daughterr zoku  0.263565\n",
       "5               quick maker  0.257749\n",
       "6               bought zoku  0.249552\n",
       "7                 quick pop  0.203686\n",
       "8                  love fun  0.199884\n",
       "9                maker love  0.180019\n",
       "10                 fun make  0.173512\n",
       "11                    quick  0.153587\n",
       "12                 make ice  0.144049\n",
       "13                ice cream  0.122350\n",
       "14                    cream  0.109916\n",
       "15                      fun  0.104427\n",
       "16                      pop  0.099035\n",
       "17                      ice  0.093253\n",
       "18                    maker  0.091205\n",
       "19                   bought  0.059113\n",
       "20                     love  0.052943\n",
       "21                     make  0.050136\n",
       "22  zzzzzzzzzzzzzzzzz night  0.000000\n",
       "23                follow go  0.000000\n",
       "24              follow goal  0.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# we use a pretrained model by google\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-002.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vector representation of word 'computer' [ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n",
      "the similarity between the words 'woman' and 'man' 0.76640123\n",
      "the most similar words to the word 'woman' [('man', 0.7664012312889099), ('girl', 0.7494640946388245), ('teenage_girl', 0.7336829900741577), ('teenager', 0.631708562374115), ('lady', 0.6288785934448242), ('teenaged_girl', 0.6141784191131592), ('mother', 0.607630729675293), ('policewoman', 0.6069462299346924), ('boy', 0.5975908041000366), ('Woman', 0.5770983099937439)]\n"
     ]
    }
   ],
   "source": [
    "print(\"the vector representation of word 'computer'\",model.wv['computer'])\n",
    "print(\"the similarity between the words 'woman' and 'man'\",model.wv.similarity('woman', 'man'))\n",
    "print(\"the most similar words to the word 'woman'\",model.wv.most_similar('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['CleanedText'].values:\n",
    "    list_of_sent.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daughter want book price amazon best alreadi tri one recip day receiv book seem happi\n",
      "*****************************************************************\n",
      "['daughter', 'want', 'book', 'price', 'amazon', 'best', 'alreadi', 'tri', 'one', 'recip', 'day', 'receiv', 'book', 'seem', 'happi']\n"
     ]
    }
   ],
   "source": [
    "print(final['CleanedText'].values[0])\n",
    "print(\"*****************************************************************\")\n",
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_count = 5 considers only words that occured atleast 5 times\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  23019\n",
      "sample words  ['daughter', 'want', 'book', 'price', 'amazon', 'best', 'alreadi', 'tri', 'one', 'recip', 'day', 'receiv', 'seem', 'happi', 'bought', 'zoku', 'quick', 'pop', 'maker', 'love', 'fun', 'make', 'ice', 'cream', 'shortag', 'avail', 'free', 'web', 'purchas', 'good', 'fruit', 'blog', 'hope', 'came', 'emphas', 'sweet', 'dessert', 'howev', 'total', 'fresh', 'fruiti', 'chapter', 'follow', 'three', 'entitl', 'scream', 'bake', 'shop', 'coco', 'might']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('son', 0.9587377309799194),\n",
       " ('granddaught', 0.9308164715766907),\n",
       " ('grandson', 0.9132821559906006),\n",
       " ('niec', 0.8836356997489929),\n",
       " ('sister', 0.8833993077278137),\n",
       " ('mom', 0.8804718255996704),\n",
       " ('boyfriend', 0.8591934442520142),\n",
       " ('nephew', 0.8586499094963074),\n",
       " ('girlfriend', 0.8392627239227295),\n",
       " ('mother', 0.8379369378089905)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('daughter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('valu', 0.6238385438919067),\n",
       " ('pricewis', 0.6231014728546143),\n",
       " ('cost', 0.6230157613754272),\n",
       " ('bargain', 0.600115180015564),\n",
       " ('pricepoint', 0.5674911737442017),\n",
       " ('expens', 0.5652279853820801),\n",
       " ('thepric', 0.5578619241714478),\n",
       " ('inexpens', 0.5563632249832153),\n",
       " ('dollar', 0.5561596155166626),\n",
       " ('buck', 0.5493758916854858)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vacat', 0.7221050262451172),\n",
       " ('hous', 0.7096303105354309),\n",
       " ('cabin', 0.6979228854179382),\n",
       " ('condo', 0.6637091636657715),\n",
       " ('visit', 0.6442372798919678),\n",
       " ('offic', 0.6426874399185181),\n",
       " ('motorhom', 0.6325644254684448),\n",
       " ('town', 0.6314110159873962),\n",
       " ('workplac', 0.6265959739685059),\n",
       " ('kitchenett', 0.6188104152679443)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teenag', 0.8579732179641724),\n",
       " ('nephew', 0.8420219421386719),\n",
       " ('grandchildren', 0.8169942498207092),\n",
       " ('granddaught', 0.8049652576446533),\n",
       " ('teen', 0.7999324798583984),\n",
       " ('boyfriend', 0.7955741882324219),\n",
       " ('grandson', 0.791936993598938),\n",
       " ('niec', 0.787968635559082),\n",
       " ('preschool', 0.7823643684387207),\n",
       " ('youngest', 0.7819018363952637)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
